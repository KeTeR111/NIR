{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a79417a6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# ПЛАН ИССЛЕДОВАНИЯ\n",
    "**Тема:** Сравнение подходов классического машинного обучения (ML) и нейронных сетей (ANN) для вычисления градиента давления в двухфазных потоках.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Отбор данных и предобработка (Data Preparation)\n",
    "**Цель:** Формирование качественного набора данных (dataset) и приведение экспериментальных параметров к критериям подобия.\n",
    "\n",
    "### 1.1. Сбор данных (Data Mining)\n",
    "* **Источники данных:** Извлечение экспериментальных точек из научных статей (базы данных по хладагентам R1234yf, R134a, R32 и CO2).\n",
    "* **Целевая переменная (Target):**\n",
    "    * Градиент давления $dP/dz$ ($kPa \\cdot m^{-1}$).\n",
    "* **Входные физические параметры:**\n",
    "    * Массовый поток $G$ ($kg \\cdot m^{-2} \\cdot s^{-1}$).\n",
    "    * Тепловой поток $\\dot{q}$ ($kW \\cdot m^{-2}$).\n",
    "    * Паросодержание $x$ (диапазон $0 \\dots 1$).\n",
    "    * Температура насыщения $T_{sat}$ ($^\\circ C$).\n",
    "    * Внутренний диаметр трубки $D$ ($mm$).\n",
    "    * Свойства жидкости и пара: плотность $\\rho_l, \\rho_v$; вязкость $\\mu_l, \\mu_v$; поверхностное натяжение $\\sigma$.\n",
    "\n",
    "### 1.2. Нормализация и обезразмеривание (Feature Engineering)\n",
    "Преобразование физических величин в безразмерные числа для обеспечения универсальности модели.\n",
    "\n",
    "* **Инерционные и вязкостные силы (Число Рейнольдса):**\n",
    "  $$Re_l, Re_v, Re_{lo}, Re_{vo}$$\n",
    "* **Инерционные силы и поверхностное натяжение (Число Вебера):**\n",
    "  $$We_l, We_v, We_{lo}, We_{vo}$$\n",
    "* **Гравитационные силы (Число Фруда):**\n",
    "  $$Fr_l, Fr_v, Fr_{lo}, Fr_{vo}$$\n",
    "* **Влияние поверхностного натяжения и гравитации (Число Бонда):**\n",
    "  $$Bo$$\n",
    "* **Параметр Локхарта-Мартинелли:**\n",
    "  $$X$$\n",
    "* **Отношения свойств фаз:**\n",
    "  $$\\rho_l / \\rho_v, \\quad \\mu_l / \\mu_v$$\n",
    "\n",
    "**Процедура:**\n",
    "1.  Расчет указанных чисел для каждой экспериментальной точки.\n",
    "2.  Нормализация данных (Z-score normalization):\n",
    "    $$z = \\frac{x - \\mu}{\\sigma}$$\n",
    "    где $\\mu$ — среднее значение, $\\sigma$ — стандартное отклонение.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Выбор моделей и архитектур (Model Selection)\n",
    "\n",
    "### 2.1. Классическое машинное обучение (ML)\n",
    "Выбор алгоритмов регрессии, способных улавливать нелинейные зависимости табличных данных:\n",
    "* **Random Forest Regressor (RF):** Для оценки важности признаков (feature importance) безразмерных чисел.\n",
    "* **Gradient Boosting (XGBoost / CatBoost):** Как основной конкурент нейросетям на табличных данных.\n",
    "* **Support Vector Regression (SVR):** Для сравнения на малых выборках.\n",
    "\n",
    "### 2.2. Нейронные сети (ANN)\n",
    "Разработка нескольких архитектур для сравнения:\n",
    "\n",
    "* **Модель A (Pure ANN):** Полносвязная сеть (Feed-forward), принимающая на вход вектор безразмерных чисел:\n",
    "  $$Inputs = [Re, We, Fr, Bo, X, \\dots]$$\n",
    "\n",
    "* **Модель B (Hybrid C-RNN):** Гибридная модель «Корреляция + Остаточная нейросеть».\n",
    "    * *Принцип:*\n",
    "      $$dP/dz_{\\text{final}} = dP/dz_{\\text{corr}} + R_{\\text{ANN}}$$\n",
    "    * *Описание:* Использование физической корреляции (например, Kim and Mudawar) как базы, сеть предсказывает только ошибку (остаток $R$).\n",
    "\n",
    "* **Архитектура:**\n",
    "    * *Входной слой:* 10–12 нейронов (по числу значимых параметров).\n",
    "    * *Скрытые слои:* Структура $[12, 8, 3]$ или $[12, 10, 8]$.\n",
    "    * *Функция активации:* $\\text{ReLU}$.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Методология анализа (Evaluation Framework)\n",
    "\n",
    "### 3.1. Функции потерь (для обучения)\n",
    "* **Mean Squared Error (MSE):** Основная функция потерь для обучения сетей.\n",
    "  $$MSE = \\frac{1}{N} \\sum_{i=1}^{N} (y_{\\text{pred}, i} - y_{\\text{exp}, i})^2$$\n",
    "* **Регуляризация:** $L2$ регуляризация ($\\lambda = 0.001$) для предотвращения переобучения.\n",
    "\n",
    "### 3.2. Метрики качества (для валидации)\n",
    "Для сравнения точности с литературными данными:\n",
    "* **Mean Percentage Absolute Error (MPAE):**\n",
    "  $$MPAE = \\frac{1}{N} \\sum_{i=1}^{N} \\left| \\frac{y_{\\text{pred}, i} - y_{\\text{exp}, i}}{y_{\\text{exp}, i}} \\right| \\times 100\\%$$\n",
    "* **Mean Percentage Error (MPE):** Для оценки систематического смещения (bias).\n",
    "  $$MPE = \\frac{1}{N} \\sum_{i=1}^{N} \\frac{y_{\\text{pred}, i} - y_{\\text{exp}, i}}{y_{\\text{exp}, i}} \\times 100\\%$$\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Этап реализации ML алгоритмов\n",
    "1.  Разделение данных: Обучающая (70%), Валидационная (15%), Тестовая (15%) выборки с фиксированным `random seed`.\n",
    "2.  Обучение моделей (RF, SVR, Boosting).\n",
    "3.  **Анализ корреляций остатков:** Вычисление корреляции Пирсона между ошибкой модели ($R$) и безразмерными числами, чтобы определить слабые места модели.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Этап реализации Нейронных сетей\n",
    "1.  **Обучение:** Использование оптимизатора Adam ($\\beta_1=0.9, \\beta_2=0.999$, learning rate $= 0.001$).\n",
    "2.  **Early Stopping:** Остановка обучения, если Loss на валидации не падает в течение заданного количества эпох или достигнут лимит (например, 1000 эпох).\n",
    "3.  **Сравнение архитектур:** Построение графика зависимости MSE от количества обучаемых параметров (Pareto front) для выбора оптимальной сложности сети.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Итоговое сравнение и валидация\n",
    "1.  **Прямой математический расчет:**\n",
    "    Сравнение прогнозов ИИ с классическими корреляциями:\n",
    "    * Kim and Mudawar.\n",
    "    * Muller-Steinhagen and Heck.\n",
    "    * Cheng et al. (для CO2).\n",
    "2.  **Сводная таблица:** Сравнение MPAE и MPE для:\n",
    "    * Лучшей ML модели.\n",
    "    * Чистой ANN.\n",
    "    * Гибридной C-RNN.\n",
    "    * Эмпирических корреляций.\n",
    "3.  **Анализ экстраполяции:** Проверка работы моделей на внешних данных, не участвовавших в обучении, для оценки переобучения."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
